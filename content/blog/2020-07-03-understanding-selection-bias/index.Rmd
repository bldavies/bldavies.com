---
title: Understanding selection bias
topics: [statistics]
---

```{r setup, echo = F, message = F, warning = F}
library(dplyr)
library(ggplot2)
library(knitr)
library(tidyr)

opts_chunk$set(message = F, warning = F,
               fig.width = 6, fig.height = 4, dpi = 100,
               fig.ext = 'svg', dev = 'svg')

theme_set(
  theme_minimal() +
    theme(panel.grid.minor = element_blank(),
          plot.title = element_text(face = 'bold'),
          plot.subtitle = element_text(margin = margin(b = 10)),
          strip.text = element_text(face = 'bold', hjust = 0, margin = margin(b = 5), size = 10))
)
```

Suppose we have data $\{(x_i,y_i):i\in\{1,2,\ldots,n\}\}$ generated by the process
$$y_i=\beta x_i+u_i,$$
where the $u_i$ are random errors with zero means, equal variances, and zero correlations with the $x_i$.
This data generating process (DGP) satisfies the [Gauss-Markov](https://en.wikipedia.org/wiki/Gauss–Markov_theorem) assumptions, so we can obtain an unbiased estimate $\hat\beta$ of the coefficient $\beta$ using ordinary least squares (OLS).

Now suppose we restrict our data to observations with $x_i\ge0$ or $y_i\ge0$.
How will these restrictions change $\hat\beta$?

To investigate, let's create some toy data:
```{r}
library(dplyr)

n <- 100
set.seed(0)
df <- tibble(x = rnorm(n), u = rnorm(n), y = x + u)
```
Here $x_i$ and $u_i$ are standard normal random variables, and $y_i=x_i+u_i$ for each observation $i\in\{1,2,\ldots,`r n`\}$.
Thus $\beta=1$.
The OLS estimate of $\beta$ is
$$\DeclareMathOperator{\Cov}{Cov}\DeclareMathOperator{\Var}{Var}\hat\beta=\frac{\Cov(x,y)}{\Var(x)},$$
where $x=(x_1,x_2,\ldots,x_{`r n`})$ and $y=(y_1,y_2,\ldots,y_{`r n`})$ are data vectors, $\Cov$ is the covariance operator, and $\Var$ is the variance operator.
For these data, we have
```{r}
cov(df$x, df$y) / var(df$x)
```
as our estimate with no selection.

Next, let's introduce our selection criteria:
```{r}
df <- df %>%
  tidyr::crossing(criterion = c('x >= 0', 'y >= 0')) %>%
  rowwise() %>%  # eval is annoying to vectorise
  mutate(selected = eval(parse(text = criterion))) %>%
  ungroup()

df
```
Now `df` contains two copies of each observation---one for each selection criterion---and an indicator for whether the observation is selected by each criterion.
We can use `df` to estimate OLS coefficients and their standard errors among observations with $x_i\ge0$ and $y_i\ge0$:
```{r}
df %>%
  filter(selected) %>%
  group_by(criterion) %>%
  summarise(n = n(),
            estimate = cov(x, y) / var(x),
            std.error = sd(y - estimate * x) / sqrt(n))
```
The OLS estimate among observations with $x_i\ge0$ approximates the true value $\beta=1$ well.
However, the estimate among observations with $y_i\ge0$ is much smaller than one.
We can confirm this visually:
```{r plot, echo = F}
lab <- c('All', 'Selected')
df %>%
  mutate(obs = lab[selected + 1]) %>%
  ggplot(aes(x, y)) +
  geom_smooth(aes(col = lab[1]), method = 'lm', se = F) +
  geom_smooth(data = filter(df, selected), aes(col = lab[2]), method = 'lm', se = F) +
  geom_point(aes(col = obs)) +
  facet_wrap(~paste('Selecting obs. with', sub('>=', '≥', criterion))) +
  labs(title = 'Selecting observations with y ≥ 0 leads to biased estimates',
       subtitle = 'Points are observations; lines are OLS best fits',
       col = 'Observations') +
  scale_colour_grey(start = 0.8, end = 0.3) +
  theme(legend.position = 'bottom')
```

What's going on?
Why do we get biased OLS estimates of $\beta$ among observations with $y_i\ge0$ but not among observations with $x_i\ge0$?

The key is to think about the errors $u_i$ in each case.
Since the $x_i$ and $u_i$ are independent, selecting observations with $x_i\ge0$ leaves the distributions of the $u_i$ unchanged---they still have zero means, equal variances, and zero correlations with the $x_i$.
Thus, the Gauss-Markov assumptions still hold and we still obtain unbiased OLS estimates of $\beta$.

```{r echo = F}
y_est <- df %>%
  filter(criterion == 'y >= 0' & selected) %>%
  mutate(u = y - x) %>%
  summarise(beta = cov(x, y) / var(x),
            rho_hat = cov(u, x) / var(x)) %>%
  as.numeric()
```

In contrast, the $x_i$ and $u_i$ are negatively correlated among observations with $y_i\ge0$.
To see why, notice that if $y_i=x_i+u_i$ then $y_i\ge0$ if and only if $x_i\ge-u_i$.
So if $x_i$ is low then $u_i$ must be high (and vice versa) for the observation to be selected.
Thus, among selected observations, we have
$$u_i=\rho x_i+\varepsilon_i,$$
where $\rho<0$ indexes (and, in this case, equals) the correlation between the $x_i$ and $u_i$, and where the residuals $\varepsilon_i$ are uncorrelated with the $x_i$.
Our DGP then becomes
$$y_i=(\beta+\rho)x_i+\varepsilon_i.$$
The $\varepsilon_i$ have equal variances (equal to $1+\rho^2$ in this case) and, again, are uncorrelated with the $x_i$.
Therefore, the OLS estimate
$$\hat\rho=\frac{\Cov(u,x)}{\Var(x)}$$
of $\rho$ is unbiased[^rho-hat], and for our toy data equals $\hat\rho\approx`r round(y_est[2], 3)`$ among observations with $y_i\ge0$.
Subtracting $\hat\rho$ from $\hat\beta$ then gives
$$\begin{align}
\hat\beta-\hat\rho
&\approx `r round(y_est[1], 3)` - (`r round(y_est[2], 3)`) \\
&= `r round(y_est[1], 3) - round(y_est[2], 3)`,
\end{align}$$
recovering the true value $\beta=1$.

[^rho-hat]: We can rewrite $\varepsilon_i=\alpha+(\varepsilon_i-\alpha)$, where $\alpha$ is the mean of the $\varepsilon_i$, and where the $(\varepsilon_i-\alpha)$ have zero means, equal variances, and zero correlations with the $x_i$.

```{r echo = F}
set.seed(0)
N <- 100
table_df <- crossing(
  criterion = c('T', 'x >= 0', 'y >= 0'),
  run = 1:N,
  obs = 1:n
) %>%
  mutate(x = rnorm(n()),
         u = rnorm(n()),
         y = x + u) %>%
  rowwise() %>%
  filter(eval(parse(text = criterion))) %>%
  group_by(criterion, run) %>%
  summarise(beta = cov(x, y) / var(x),
            rho = cov(u, x) / var(x)) %>%
  ungroup() %>%
  mutate(diff = beta - rho) %>%
  gather(key, value, beta, rho, diff) %>%
  group_by(criterion, key) %>%
  summarise(mean = mean(value),
            sd = sd(value),
            n = n()) %>%
  ungroup() %>%
  mutate(ci_radius = qt(1 - 0.05 / 2, n - 1) * sd / n)
```

The table below reports 95% confidence intervals for $\hat\beta$, $\hat\rho$, and $(\hat\beta-\hat\rho)$, estimated by simulating the DGP $y_i=x_i+u_i$ described above `r N` times.
The table confirms that the OLS estimate $\hat\beta$ of $\beta=1$ is unbiased among observations with $x_i\ge0$ but biased negatively among observations with $y_i\ge0$.
```{r echo = F}
table_df %>%
  mutate(ci = sprintf('%.3f \u00B1 %.3f', mean, ci_radius)) %>%
  select(criterion, key, ci) %>%
  mutate(key = factor(key, levels = c('beta', 'rho', 'diff'))) %>%
  spread(key, ci) %>%
  mutate(criterion = c('All', 'With $x_i\\ge0$', 'With $y_i\\ge0$')) %>%
  kable(align = 'lccc', col.names = c('Observations', '$\\hat\\beta$', '$\\hat\\rho$', '$\\hat\\beta-\\hat\\rho$'))
```

The estimate $\hat\beta$ always differs from $\beta$ by $\hat\rho$, which is significantly non-zero among observations with $y_i\ge0$.
However, this pattern is not useful empirically because we generally don't observe the $u_i$ and so can't estimate $\hat\rho$ to back out the true value of $\beta=\hat\beta-\hat\rho$.
Instead, we may use the [Heckman correction](https://en.wikipedia.org/wiki/Heckman_correction) to adjust for the bias introduced through non-random selection.

In empirical settings, selecting observations with $x_i\ge0$ may lead to biased estimates when (i) there is heterogeneity in the relationship between $y_i$ and $x_i$ across observations $i$, and (ii) OLS is used to estimate an [average treatment effect](https://en.wikipedia.org/wiki/Average_treatment_effect).[^shakked]
In particular, if the $x_i$ are correlated with the observation-specific treatment effects then restricting to observations with $x_i\ge0$ changes the distribution, and hence the mean, of those effects non-randomly.

[^shakked]: Thanks to [Shakked](https://motu.nz/about-us/people/shakked-noy/) for pointing this out.

```{r session-info, echo = F}
bldr::save_session_info()
```
