---
title: Sampling the Motu Coauthorship Network
tags: [Motu, networks, R, sampling]
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(dev = 'svg', fig.ext = 'svg', message = FALSE, warning = FALSE)
```

Suppose I have some data that describe a bipartite author-publication network.
I want to analyse the underlying coauthorship network---that is, the bipartite projection onto the set of authors---but I can't compute that network because the data are too large to fit into memory.
Instead, I estimate [properties](https://en.wikipedia.org/wiki/Graph_property) of the full coauthorship network by sampling the author-publication incidence data before computing the bipartite projection.

If the incidence data are stored as a matrix then I can sample its rows or columns, which corresponds to sampling the author or publication sets.
If the incidence data are stored as a list of author-publication pairs then I can sample these pairs, which corresponds to sampling edges in the bipartite network.

Which of these three methods---author, publication and edge sampling---most reliably estimates the full coauthorship network's properties?

To develop some intuition, I apply each sampling method to [the coauthorship network among Motu researchers](/blog/coauthorship-networks-motu/).
The [data](https://github.com/bldavies/motuwp) describing this network are small enough that I can compute the true values of various network properties, which I compare with the sampling distributions of such values generated by each sampling method.

The table below reports the 95% confidence intervals for each property under each method, in all cases sampling (uniformly at random and without replacement) about half of the corresponding entities (i.e., authors, publications or edges) before computing the bipartite projection onto the set of authors.[^deletion]

```{r echo = FALSE}
# Load packages
library(dplyr)
library(ggplot2)
library(igraph)
library(readr)
library(tidyr)

# Import data
data <- read_csv('https://raw.githubusercontent.com/bldavies/motuwp/8f4b1c02e04f8e5e45b4325195bb4f03ac0ee707/data/authors.csv')

# Define function for computing network properties
get_properties <- function(inc_mat) {
  net <- (inc_mat %*% t(inc_mat)) %>%
    graph.adjacency(mode = 'undirected') %>%
    simplify()
  tibble(
    Order = gorder(net),
    Size  = gsize(net),
    `Density (%)` = 100 * graph.density(net),
    `Transitivity (%)` = 100 * transitivity(net),
    `Mean distance` = mean_distance(net, directed = FALSE)
  )
}

# Define function for getting properties of sample networks
get_sample_properties <- function(tbl, sample_size, method = 'author') {
  if (method %in% c('author', 'pub')) {
    inc_mat <- table(tbl$author, tbl$paper)
    if (method == 'author') {
      inc_mat[sample(1:nrow(inc_mat), sample_size), ] %>%
        {.[, which(colSums(.) > 0)]} %>%
        get_properties()
    } else {
      inc_mat <- inc_mat[, sample(1:ncol(inc_mat), sample_size)] %>%
        {.[which(rowSums(.) > 0), ]} %>%
        get_properties()
    }
  } else {
    tbl %>%
      sample_n(sample_size) %>%
      {table(.$author, .$paper)} %>%
      get_properties()
  }
}

# Define functions for computing samples
get_samples <- function(n_samples, sample_size, func) {
  bind_rows(lapply(1 : n_samples, func))
}
get_author_samples <- function(n_samples, sampling_rate) {
  sample_size <- round(sampling_rate * n_distinct(data$author))
  func <- function(x) get_sample_properties(data, sample_size)
  get_samples(n_samples, sample_size, func) %>%
    mutate(sampling_rate = sampling_rate)
}
get_pub_samples <- function(n_samples, sampling_rate) {
  sample_size <- round(sampling_rate * n_distinct(data$paper))
  func <- function(x) get_sample_properties(data, sample_size, method = 'pub')
  get_samples(n_samples, sample_size, func) %>%
    mutate(sampling_rate = sampling_rate)
}
get_edge_samples <- function(n_samples, sampling_rate) {
  sample_size <- round(sampling_rate * nrow(data))
  func <- function(x) get_sample_properties(data, sample_size, method = 'edge')
  get_samples(n_samples, sample_size, func) %>%
    mutate(sampling_rate = sampling_rate)
}

# Compute samples
n_samples <- 100
sampling_rates <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)
set.seed(0)
author_samples <- lapply(sampling_rates, function(x) get_author_samples(n_samples, x)) %>%
  bind_rows() %>%
  mutate(method = 'Author sampling')
pub_samples <- lapply(sampling_rates, function(x) get_pub_samples(n_samples, x)) %>%
  bind_rows() %>%
  mutate(method = 'Pub. sampling')
edge_samples <- lapply(sampling_rates, function(x) get_edge_samples(n_samples, x)) %>%
  bind_rows() %>%
  mutate(method = 'Edge sampling')

# Tabulate true values and 95% confidence intervals
ci_data <- rbind(author_samples, pub_samples, edge_samples) %>%
  gather(Property, value, -method, -sampling_rate) %>%
  group_by(method, sampling_rate, Property) %>%
  summarise(mean = mean(value, na.rm = TRUE),
            st_dev = sd(value, na.rm = TRUE),
            n = sum(!is.na(value))) %>%
  ungroup() %>%
  mutate(ci_radius = qt(1 - 0.05 / 2, n - 1) * st_dev / sqrt(n),
         ci_lower = mean - ci_radius,
         ci_upper = mean + ci_radius)
tmp <- ci_data %>%
  filter(sampling_rate == 0.5) %>%
  select(-sampling_rate) %>%
  mutate_if(is.numeric, function(x) round(x, 1)) %>%
  mutate(value = sprintf('%.2f \u00B1 %.2f', mean, ci_radius)) %>%
  select(method, Property, value)
get_properties(table(data$author, data$paper)) %>%
  gather(Property, value) %>%
  mutate(value = sprintf('%.2f', value)) %>%
  mutate(method = 'True value') %>%
  select(method, Property, value) %>%
  rbind(tmp) %>%
  mutate(Property = factor(Property, levels = c('Order', 'Size', 'Density (%)', 'Mean distance', 'Transitivity (%)')),
         method = factor(method, levels = c('True value', 'Author sampling', 'Pub. sampling', 'Edge sampling'))) %>%
  spread(method, value) %>%
  knitr::kable(align = 'lrrrr')
```

All three methods under-estimate the order and size of the full coauthorship network.
However, this is partly by construction: sampling any proportion of authors will always deliver that proportion of nodes in the coauthorship network, and taking a strict subset of publications or edges will generally omit some inter-author connections.

Author and publication sampling deliver accurate density and transitivity estimates.
Edge sampling is less accurate: it produces relatively sparse networks in which authors are more distant, and less likely to share common coauthors, than in the full network.

The chart below plots the sample means and 95% confidence intervals generated by each sampling method for varying sampling rates.
(A sampling rate of p% means that I randomly select p% of the corresponding entities before computing the coauthorship network.)
As the sampling rate rises, the sample means converge to the true value.
I vertically nudge the plotted points to prevent overlaps and make it easier to compare methods at each sampling rate.

```{r convergence, echo = FALSE, fig.width = 8, fig.height = 6}
library(ggplot2)

# Compute true values
baselines <- get_properties(table(data$author, data$paper)) %>%
  select(-Order, -Size) %>%
  gather(Property, value)

# Plot 95% CIs at varying sampling rates
ci_data %>%
  filter(!Property %in% c('Order', 'Size')) %>%
  mutate(method = factor(method, levels = c('Author sampling', 'Pub. sampling', 'Edge sampling'))) %>%
  ggplot(aes(y = sampling_rate + (as.numeric(method) - 2) / 60, col = method)) +
  geom_point(aes(x = mean)) +
  geom_errorbarh(aes(xmin = mean - ci_radius, xmax = mean + ci_radius), height = 0.05) +
  geom_vline(data = baselines, aes(xintercept = value)) +
  facet_wrap(~Property, scales = 'free_x') +
  labs(x = 'Value',
       y = 'Sampling rate (%)',
       col = 'Method',
       title = 'Comparing estimates between sampling methods',
       subtitle = 'Error bars represent 95% CIs and vertical lines represent true values') +
  scale_y_continuous(breaks = 0.2 * (1 : 5), labels = function(x) 100 * x) +
  theme_minimal(base_size = 11) +
  theme(legend.position = 'bottom',
        panel.grid.minor = element_blank(),
        plot.subtitle = element_text(margin = margin(b = 10), size = 13),
        plot.title = element_text(face = 'bold', margin = margin(b = 10), size = 16),
        strip.text = element_text(face = 'bold', hjust = 0, margin = margin(b = 5), size = 12))
```

Publication sampling over-estimates the coauthorship network's density at low sampling rates.
This could be because most working papers are written by authors in the densely connected core of the coauthorship network, so publication sampling is more likely to recover this core than the less connected and less productive periphery.

Edge sampling appears to generate biased density and transitivity estimates.
Intuitively, pairs of sampled edges are unlikely to be incident with the same publication and thus unlikely to form an edge in the bipartite projection.

All three methods under-estimate the mean distance between authors at low sampling rates but over-estimate this distance at high sampling rates.
This pattern arises because the distance calculation considers connected nodes only.
At low sampling rates, most connected components are dyads or triads, and so the distances between connected nodes are small.
The number of nodes in each component rises with the sampling rate, which leads to mean distance over-estimates until the number of edges within each component catches up.

[^deletion]: Within each sample, I delete authors with no publications and publications with no authors.

```{r session-info, echo = FALSE}
writeLines(capture.output(sessioninfo::session_info()), 'session.log')
```
