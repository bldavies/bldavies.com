---
title: Reading the Ministerial Diaries
topics: [politics]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = "svg",
                      echo = TRUE,
                      fig.ext = "svg",
                      message = FALSE,
                      warning = FALSE)
```

In December 2018, the New Zealand Government [announced](https://www.beehive.govt.nz/release/government-proactively-release-ministerial-diaries) that its ministers "will for the first time release details of their internal and external meetings."
The Government has since published these "ministerial diaries" as [a series of PDFs](https://www.beehive.govt.nz/search?f%5B0%5D=content_type_facet%3Aministerial_diary&f%5B1%5D=government_facet%3A6203&f%5B2%5D=ministers%3A6205).
In this post, I analyse the ministerial diary of [David Parker](https://www.beehive.govt.nz/minister/hon-david-parker), a ["pivotal cabinet minister"](https://www.odt.co.nz/news/election-2017/parker-emerges-pivotal-cabinet-minister) who wears a range of politically and economically significant hats:

* Attorney-General;
* Minister of Economic Development;
* Minister for the Environment;
* Minister of Trade and Export Growth;
* Associate Minister of Finance.

These roles, coupled with his scheduled activities for the 2018 calendar year being available in [a single, consistently formatted table](https://www.beehive.govt.nz/sites/default/files/2019-05/October%202017%20-%20December%202018_0.pdf), make Minister Parker's diary (hereafter "the diary") an interesting and relatively painless document to analyse.

## Parsing the data

I read the diary into R using the `pdf_data` function from [`pdftools`](https://cran.r-project.org/package=pdftools):

```{r import, eval=FALSE}
library(pdftools)

path <- "https://www.beehive.govt.nz/sites/default/files/2019-05/October%202017%20-%20December%202018_0.pdf"
pages <- pdf_data(path)
```

```{r save-cache, echo=FALSE, eval=FALSE}
saveRDS(pages,"data/pages.rds")
```

```{r load-cache, echo=FALSE, eval=TRUE}
pages <- readRDS("data/pages.rds")
```

`pdf_data` scans each page for distinct words, encloses these words in [bounding boxes](https://en.wikipedia.org/wiki/Minimum_bounding_box), and stores the coordinates and content of each box as a list of tibbles.
For example, the diary's first page contains the following data:

```{r first-page}
library(dplyr)

pages[[1]]
```

The `x` and `y` columns provide the horizontal and vertical displacement, in pixels, of each bounding box from the top-left corner of the page.
The left-most boxes sit 72 pixels from the left page boundary, allowing me to identify table rows by the cumulative number of boxes for which `x` equals 72.

```{r identify-rows}
pages[[1]] %>%
  arrange(y, x) %>%
  mutate(row = cumsum(x == 72)) %>%
  filter(cumsum(x == 72 & text == "Date") > 0)  # Remove preamble
```

The `x` values for which `row` equals 14 provide the left alignment points for the text in each of the diary's six columns.
These points remain unchanged across all 84 pages, allowing me to identify rows and columns throughout the diary within a single pipe:

```{r define-clean_data, echo=FALSE}
clean_data <- function (df) {
  df %>%
    # Replace non-ASCII characters with ASCII equivalents
    mutate(text = iconv(text, "", "ASCII", sub = "byte"),
           text = gsub("<c3><a7>", "c", text),
           text = gsub("<c3><a9>", "e", text),
           text = gsub("<c3><b1>", "n", text),
           text = gsub("<c4><81>", "a", text),
           text = gsub("<c5><ab>", "u", text),
           text = gsub("<e2><80><93>", "-", text),
           text = gsub("<e2><80><99>", "'", text),
           text = gsub("<e2><80><9c>|<e2><80><9d>", "\"", text)) %>%
    # Fix linebroken data ranges
    spread(column, text) %>%
    mutate(split_date = is.na(scheduled_time) & grepl("-", paste(date, lag(date))),
           row = cumsum(!split_date)) %>%
    select(-split_date) %>%
    gather(column, text, -row) %>%
    group_by(row, column) %>%
    summarise(text = gsub("NA", "", paste(text, collapse = " "))) %>%
    ungroup() %>%
    mutate(text = trimws(text),
           text = ifelse(text == "", NA, text)) %>%
    # Fix transcription errors
    mutate(text = gsub("Minster", "Minister", text),
           text = ifelse(column == "portfolio" & text == "Minister Little", "Attorney-General", text))
}
```

```{r define-diary}
library(tidyr)

# Define column names and left alignment points
columns <- tibble(
  left_x = c(72, 149, 235, 390, 504, 630),
  name = c("date", "scheduled_time", "meeting", "location", "with", "portfolio")
)

# Identify page numbers
for (i in 1 : length(pages)) pages[[i]]$page <- i

# Process data
diary <- bind_rows(pages) %>%
  # Identify table rows
  arrange(page, y, x) %>%
  mutate(row = cumsum(x == columns$left_x[1])) %>%
  filter(cumsum(x == columns$left_x[1] & text == "Date") == 1) %>%
  filter(row > min(row)) %>%  # Remove header row
  # Identify table columns
  mutate(column = sapply(x, function(x){max(which(columns$left_x <= x))}),
         column = columns$name[column]) %>%
  # Concatenate text within table cells
  group_by(row, column) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup() %>%
  # Clean data
  clean_data() %>%
  # Convert to wide format
  mutate(column = factor(column, levels = columns$name)) %>%
  spread(column, text) %>%
  select(-row)
```

I define the `clean_data` function in [the appendix](#appendix) below.

The resulting tibble `diary` contains 1,553 rows, each of which describes a unique entry scheduled between October 2017 and December 2018.
I select entries scheduled during the 2018 calendar year:

```{r define-data}
(data <- filter(diary, grepl("2018", date)))
```

According to [the official disclaimer](https://www.beehive.govt.nz/ministerial-diaries-full-disclaimer), the diary excludes personal and party political meetings, along with details published elsewhere such as time spent in the House of Representatives.
Moreover, some details are withheld under various sections of [the Official Information Act](http://legislation.govt.nz/act/public/1982/0156/latest/DLM64785.html).
I assume that the remaining entries provide a representative sample of Minister Parker's ministerial activities.

## Analysing word frequencies

I analyse the frequency of words used in the `with` column of `data`.
These frequencies provide insight into Minister Parker's interactions with different organisations.
I use the `unnest_tokens` function from [`tidytext`](https://cran.r-project.org/package=tidytext) to identify unique words and the `count` function from `dplyr` to count word frequencies.

```{r word-frequencies}
library(tidytext)

data %>%
  unnest_tokens(word, with) %>%
  anti_join(get_stopwords()) %>%  # Remove stop words
  count(word, sort = TRUE)
```

The most frequent word, "attending," reflects cabinet meetings, media briefings and other general ministerial duties.
The next most frequent word, "officials," reflects Minister Parker's meetings with the Ministry for the Environment (MfE), the Ministry of Business, Innovation and Employment (MBIE), and the Ministry of Foreign Affairs and Trade (MFAT), along with other government departments.
Both "minister" and "ministers" reflect meetings with Ministers [Jones](https://www.beehive.govt.nz/minister/hon-shane-jones), [Sage](https://www.beehive.govt.nz/minister/hon-eugenie-sage), [Twyford](https://www.beehive.govt.nz/minister/hon-phil-twyford) and others.

### Computing tf-idf scores

Counting word frequencies across all portfolios masks portfolio-specific interactions.
I infer such interactions from the [*term frequency-inverse document frequency*](https://www.tidytextmining.com/tfidf.html) (tf-idf) scores of word-portfolio pairs.
I identify these pairs as follows.

```{r}
word_portfolio_pairs <- data %>%
  # Disambiguate portfolio names
  mutate(portfolio = gsub("Att.*?ral|AG", "Attorney-General", portfolio)) %>%
  # Split entries with multiple porfolios
  mutate(portfolio = gsub("[^[:alpha:] -]", "&", portfolio),
         portfolio = strsplit(portfolio, "&")) %>%
  unnest() %>%
  mutate(portfolio = trimws(portfolio)) %>%
  # Identify word-portfolio pairs
  filter(!is.na(portfolio)) %>%
  unnest_tokens(word, with) %>%
  select(word, portfolio)
```

tf-idf scores measure the "importance" of words in each document in a corpus.
The *term frequency*

$$\mathrm{tf}(w, d)=\frac{\text{Number of occurrences of word}\ w\ \text{in document}\ d}{\text{Number of words in document}\ d}$$

measures the rate at which word $w$ occurs in a document $d$, while the *inverse document frequency*

$$\mathrm{idf}(w) = -\ln\left(\frac{\text{Number of documents containing word}\ w}{\text{Number of documents}}\right)$$

provides a normalisation factor that penalises ubiquitous words.
The tf-idf score

$$\text{tf-idf}(w,d) = \mathrm{tf}(w, d) \cdot \mathrm{idf}(w)$$

thus measures the prevalence of word $w$ in document $d$, normalised by that word's prevalence in other documents.
I interpret the set of entries associated with each portfolio as a document and use the `bind_tf_idf` function from `tidytext` to compute word-portfolio tf-idf scores:

```{r tf-idf-scores}
word_portfolio_pairs %>%
  count(word, portfolio) %>%
  bind_tf_idf(word, portfolio, n)
```

The `idf` column identifies both language-specific stop words (e.g., "a") and context-specific stop words (e.g., "advisory") that are common across portfolios.

The chart below presents the highest tf-idf words for each portfolio.
These words reveal organisations (e.g., the Parliamentary Counsel Office) and individuals (e.g., [Cecilia MalmstrÃ¶m](https://ec.europa.eu/commission/commissioners/2014-2019/malmstrom_en)) that are missing from the diary-wide word frequencies computed above.

```{r highest-tf-idf, echo=FALSE, fig.width=8, fig.height=6}
library(ggplot2)

word_portfolio_pairs %>%
  count(word, portfolio) %>%
  bind_tf_idf(word, portfolio, n) %>%
  group_by(portfolio) %>%
  top_n(5, tf_idf) %>%
  filter(n > 1) %>%
  ggplot(aes(reorder(paste(word, portfolio, sep = "_"), tf_idf), tf_idf)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~portfolio, scales = "free") +
  labs(x = NULL,
       y = "tf-idf",
       title = "Highest tf-idf words by portfolio") +
  scale_x_discrete(labels = function(x) gsub("_.+$", "", x), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold", margin = margin(b = 10), size = 16))
```

The chart also reveals which interactions correspond to which portfolios.
For example, Minister Parker's frequent interactions with MBIE officials appear to be most associated with the Economic Development portfolio, while his interactions with Minister Sage appear to involve both the Environment and Associate Finance portfolios.
([Minister Sage's diary](https://www.greens.org.nz/sites/default/files/Eugenie%20Sage%27s%20July-Sept%202018%20Diary.pdf) suggests that such cross-portfolio interactions relate to the Overseas Investment Office, for which Ministers Parker and Sage are jointly responsible.)

##  Acknowledgements

[The pdftools 2.0 release notes](https://ropensci.org/technotes/2018/12/14/pdftools-20/) helped me interpret `pdf_data`'s output.
[Julia Silge](https://juliasilge.com) and [David Robinson](http://varianceexplained.org)'s book [*Text Mining with R*](https://www.tidytextmining.com) provided useful background reading, especially [the chapter on tf-idf scores](https://www.tidytextmining.com/tfidf.html).

## Appendix

### Source code for `clean_data()`

```{r appendix, ref.label='define-clean_data'}
```

```{r session-info, echo = FALSE}
writeLines(capture.output(sessioninfo::session_info()), 'session.log')
```
