---
title: Snowball sampling bias in program evaluation
topics: [networks, statistics]
---

```{r setup, echo = F, message = F, warning = F}
# Load packages
library(bldr)
library(dplyr)
library(ggplot2)
library(igraph)
library(knitr)
library(purrr)
library(tidyr)
library(xfun)

# Set ggplot theme
set_ggtheme()

# Set knitr options
opts_chunk$set(echo = F, message = F,
               fig.width = 6, fig.height = 3.5, fig.ext = 'svg', dev = 'svg')

# Generate network
set.seed(0)
G = sample_gnm(1000, 1000)

# Set simulation parameters
n = gorder(G) / 4
rho_vals = (0:5) / 5
n_runs = 500

# Define function for running simulations
simulate = function(d, n, rho = 0.5) {
  
  # Extract population and sample sets, and assign treatments
  P = seq_along(d)
  S = sort(sample(P, n, prob = d / sum(d)))
  t = round(runif(n))
  
  # Simulate treatment effects and outcomes
  r = (rho ^ 2 - sqrt(rho ^ 2 * (1 - rho ^ 2))) / (2 * rho ^ 2 - 1)
  b = 1 - r * as.numeric(scale(d)) + (1 - r) * rnorm(length(d))
  y = b[S] * t + rnorm(n)
  
  # Return results
  tibble(i = S, t, b = b[S], y, d = d[S])
}

# Run simulations
set.seed(0)
sims = crossing(n = n, rho = rho_vals, run = 1:n_runs) %>%
  mutate(res = map2(n, rho, ~simulate(degree(G), ..1, ..2))) %>%
  unnest('res')

get_ate = function(y, t, w = NULL) {
  if (is.null(w)) w = rep(1, length(y))
  sum(y * t * w) / sum(t * w) - sum(y * (1 - t) * w) / sum((1 - t) * w)
}
```

Suppose I want to run a pilot study of a mental health support program before rolling it out at scale.
The program has heterogeneous treatment effects, but tends to be more effective for people who have fewer social connections.
Such people tend to have lower mental health ([Kawachi and Berkman, 2001](https://doi.org/10.1093/jurban/78.3.458)) and so have more to gain from participating in the program.

I recruit people to my study via [snowball sampling](https://en.wikipedia.org/wiki/Snowball_sampling): I advertise it to a few initial seeds, who share the ads with their friends, who share the ads with their friends, and so on.
Everyone who sees an ad participates.
But some people are more likely to see ads than others: in particular, people with more friends have more chances to be sent an ad.
Consequently, I will tend to under-estimate the average treatment effect (ATE) of the program because people with more social connections, for whom the program is less effective, are more likely to appear in my pilot sample.
Such under-estimation may lead me to abandon the program even if its mental health benefits actually outweigh its implementation costs.

## Demonstration

As a concrete example, suppose each individual $i$ has degree $d_i$ in the social network from which I recruit my sample.
The treatment effect of the program on individual $i$ is
$$\beta_i=1-r\bar{d}_i+(1-r)z_i,$$
where $\bar{d}_i$ is a normalization of $d_i$ with zero mean and unit variance across the network, the $z_i$ are iid standard normal, and $r$ is a parameter controlling the (negative) correlation between the $\beta_i$ and $d_i$.
The treatment effects $\beta_i$ give rise to individual-level outcomes
$$y_i=\beta_it_i+\epsilon_i,$$
where the $t_i$ are binary treatment indicators and the $\epsilon_i$ are iid standard normal errors.
The sample delivers an estimate
$$\hat\beta=\frac{\sum_iy_it_i}{\sum_it_i}-\frac{\sum_iy_i(1-t_i)}{\sum_i(1-t_i)}$$
of the program's ATE: the difference in mean outcomes between treated and untreated members of the pilot sample.
Treatments are assigned to sample members randomly.
But the sample is recruited non-randomly: individual $i$ is recruited with probability proportional to their degree $d_i$.
This non-random recruitment leads to sampling bias when the $\beta_i$ and $d_i$ are correlated.

The chart below summarizes the distribution of ATE estimates across `r n_runs` snowball samples of `r n` people from a random social network.
This network contains `r format(gsize(G), big.mark = ',')` bilateral friendships among `r format(gorder(G), big.mark = ',')` people.
Network degrees vary between `r numbers_to_words(min(degree(G)))` and `r numbers_to_words(max(degree(G)))`, producing variation in the probability of being sampled.
I randomize the treatment effects $\beta_i$ and assignments $t_i$ in each simulation run.

```{r example}
sims %>%
  group_by(rho, run) %>%
  summarise(est = get_ate(y, t)) %>%
  group_by(rho) %>%
  summarise(mean = mean(est),
            ci_lower = quantile(est, 0.025),
            ci_upper = quantile(est, 0.975)) %>%
  ggplot(aes(rho)) +
  geom_hline(yintercept = 1, linetype = 'dashed') +
  geom_point(aes(y = mean)) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
  labs(x = 'Correlation of treatment effects and network degrees',
       y = 'ATE estimate',
       title = 'Snowball sampling can lead to biased ATE estimates',
       subtitle = 'Error bars denote 95% confidence intervals; dashed line denotes true ATE') +
  scale_x_continuous(breaks = (0:5) / 5) +
  theme(legend.justification = c(0, 0),
        legend.position = c(0, 0))
```

The ATE estimate is unbiased when treatment effects are uncorrelated with network degrees.
However, the estimate becomes more biased as the correlation becomes stronger.
Intuitively, the more the program's effectiveness is concentrated among low-degree individuals, the worse the program looks in pilot samples excluding those individuals (independently of how treatments are assigned).

## Potential solutions

How can we mitigate snowball sampling bias?
One approach is to collect information about sample members' degrees in the social network, and use this information to obtain weighted ATE estimates.[^rds]
The difference-in-means estimator $\hat\beta$ equals the OLS estimator of $\beta$ in the linear model
$$y_i=\beta t_i+\varepsilon_i$$
relating outcomes to treatment assignments.
Using [weighted least squares](https://en.wikipedia.org/wiki/Weighted_least_squares) (WLS) with weights $w_i=1/\sqrt{d_i}$ may deliver less biased estimates by accounting for the probability of sampling each individual $i$.
Intuitively, individuals with lower degrees provide relatively more information about the true ATE because they are less likely to be sampled, and so giving these individuals higher weights in the estimation procedure leads to more informed estimates.[^sqrt]
However, the distribution of degrees $d_i$ in the sample is different than the distribution of degrees in the full network, and so weighting by the (observed) $d_i$ may still deliver different (and thus incorrect) estimates than weighting by the (unobserved) sampling probabilities.

[^rds]: This approach is conceptually similar to the "respondent-driven sampling" technique described by [Salganik and Heckathorn (2004)](https://doi.org/10.1111/j.0081-1750.2004.00152.x).

[^sqrt]: Taking square roots recognizes that the objective of WLS is to minimize the weighted sum of *squared* residuals.

Another approach, suggested by [Jackson et al. (2020)](https://dx.doi.org/10.2139/ssrn.3522256), is to model sample recruitment explicitly using game theory.
The authors describe a game wherein each individual's recruitment payoff depends on whether their peers are recruited.
The equilibrium of this game determines each individual's recruitment probability conditional on the network structure (and other covariates).
Jackson et al. embed this game in an estimation procedure based on [propensity score matching](https://en.wikipedia.org/wiki/Propensity_score_matching), and show theoretically and empirically that this procedure leads to better ATE estimates.

```{r session-info}
save_session_info()
```

---

*Thanks to [Ryan Brennan](https://twitter.com/RyanBrennanEcon) for discussing the ideas presented in this post.*
