---
title: Selection bias and fixed effects
topics: [statistics]
summary: |
  Including fixed effects in a regression equation does not remove selection bias if the selection criteria are nonlinear in the dependent variable.
---

```{r setup, echo = F, message = F}
library(bldr)
library(dplyr)
library(ggplot2)
library(knitr)
library(lfe)
library(purrr)
library(tidyr)
library(xfun)

opts_chunk$set(echo = F, message = F,
               fig.width = 6, fig.height = 3, fig.ext = 'svg', dev = 'svg')

set_ggtheme()
```

```{r get_data, cache = T}
get_data = function(ni = 100, nt = 10) {
  u = rnorm(ni)
  # u = u - mean(u)
  crossing(i = 1:ni, t = 1:nt) %>%
    mutate(x = rnorm(n()),
           u = u[i],
           eps = rnorm(n()),
           y = x + u + eps) %>%
    group_by(i) %>%
    mutate(y_mean = mean(y),
           y_sq_mean = mean(y ^ 2)) %>%
    ungroup()
}
```

Economists often use [fixed effects](https://en.wikipedia.org/wiki/Fixed_effects_model) to correct for [selection bias](/blog/understanding-selection-bias).
Intuitively, these effects "partial out" the reasons why our data include some observations but not others.
But this intuition relies on the selection criteria being linear functions of the dependent variable.

```{r dat}
ni = 100
nt = 10

set.seed(ni)
dat = get_data()
```

For example, suppose I have panel data on `r ni` individuals $i$ at `r n2w(nt)` dates $t$.
These data include pairs $(y_{it},x_{it})$ generated by the process
$$y_{it}=x_{it}+u_i+\epsilon_{it},$$
where $u_i$ is a fixed effect and $\epsilon_{it}$ is an error term.
The $x_{it}$, $u_i$, and $\epsilon_{it}$ are iid normal with zero mean and unit variance.
They all vary across individuals.
The $x_{it}$ and $\epsilon_{it}$ also vary over time, but the $u_i$ do not.

The chart below plots $y_{it}$ against $x_{it}$ overall and within two subsets of my data:

1. Observations for the `r ni / 2` individuals $i$ whose outcomes $y_{it}$ have the largest mean;
2. Observations for the `r ni / 2` individuals $i$ whose *squared* outcomes $y_{it}^2$ have the largest mean.

It also shows the OLS regression line fitted to my data and its subsets.
The intercept and slope of this line depend on the selection criterion.
Individuals with larger mean outcomes tend to have larger fixed effects and narrower error distributions.
This leads OLS to estimate a higher intercept but shallower slope than in the full data.
In contrast, individuals with larger mean squared outcomes have similar fixed effects to other individuals but wider error distributions.
This leads OLS to estimate the same intercept but steeper slope than in the full data.

```{r keys, cache = T}
keys = c('All data', 'Largest means', 'Largest mean squares')
```

```{r binscatter, fig.height = 3.5}
plot_df = bind_rows(
  mutate(dat, key = keys[1], included = T),
  mutate(dat, key = keys[2], included = y_mean > median(y_mean)),
  mutate(dat, key = keys[3], included = y_sq_mean > median(y_sq_mean)),
) %>%
  mutate(key = factor(key, keys))

plot_df_rounded = plot_df %>%
  mutate(across(c(x, y), ~round(. / 0.5) * 0.5)) %>%
  count(key, x, y, included)

plot_df_rounded %>%
  filter(included) %>%
  ggplot(aes(x, y)) +
  geom_point(aes(alpha = n), show.legend = F) +
  geom_smooth(data = filter(plot_df, included), method = 'lm', col = 'black', se = F) +
  geom_abline(intercept = 0, slope = 1, lty = 'dashed') +
  facet_wrap(~key) +
  labs(title = 'Distribution of my data and its subsets',
       subtitle = 'Darker points represent more common rounded values\nSolid lines represent OLS best fits\nDashed lines represent true relationship in full data') +
  coord_cartesian(clip = 'off') +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme(legend.justification = c(1, 0),
        legend.position = c(1, 0))
```

```{r get_estimates, cache = T}
get_estimates = function(d) {
  list(
    OLS = coef(.lm.fit(matrix(c(rep(1, nrow(d)), d$x), ncol = 2), d$y))[2],
    FE = coef(lfe::felm(y ~ x | i, data = d))[['x']]
  )
}
```

```{r simulate_one, cache = T, dependson = c('get_data', 'get_estimates', 'keys')}
simulate_one = function(ni = 100, nt = 10) {
  dat = get_data(ni, nt)
  dat_mean = filter(dat, y_mean >= median(y_mean))
  dat_sq_mean = filter(dat, y_sq_mean >= median(y_sq_mean))
  map_df(list(dat, dat_mean, dat_sq_mean), get_estimates) %>%
    mutate(key = factor(keys, keys))
}
```

```{r sims, cache = T, dependson = 'simulate_one'}
n_runs = 100

set.seed(0)
sims = crossing(run = 1:n_runs) %>%
  mutate(res = map(run, ~simulate_one())) %>%
  unnest('res')
```

What if I include fixed effects in my regression?
The box plots below summarize the slopes I estimate when I simulate my data `r n_runs` times and apply my selection criteria.
Including fixed effects removes the bias from selecting on mean outcomes.
This is because the fixed effects *are* the variables I select on.
Partialing them out removes the selection bias by definition.
In contrast, including fixed effects does not remove the bias from selecting on mean squared outcomes.
This is because the fixed effects are uncorrelated with the variables I select on.
Partialing them out removes noise but not bias.

```{r boxplot}
estimators = c('With', 'Without')
sims %>%
  gather(estimator, value, -key, -run) %>%
  mutate(estimator = factor(estimators[(estimator == 'OLS') + 1], estimators)) %>%
  ggplot(aes(key, value)) +
  geom_boxplot(aes(col = estimator)) +
  geom_hline(yintercept = 1, lty = 'dashed') +
  labs(x = NULL,
       y = 'Estimate',
       title = 'OLS estimates with and without fixed effects',
       subtitle = 'Dashed line represents true value',
       col = NULL) +
  guides(col = guide_legend(ncol = 2)) +
  coord_cartesian(expand = F, clip = 'off') +
  scale_color_grey(start = 1/3, end = 2/3) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0, 1))
```

```{r session-info, echo = F}
save_session_info()
```
