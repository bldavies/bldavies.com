---
title: Models of the AI apocalypse
topics: [research, technology]
summary: You can't beat a model by waffling.
---

In [this week's episode of *EconTalk*](https://www.econtalk.org/tyler-cowen-on-the-risks-and-impact-of-artificial-intelligence/), Tyler Cowen asks:

> "Is there any actual mathematical model of this process of how the world is supposed to end?
> ...
> If you look, say, at COVID or climate change fears, in both cases, there are many models you can look at.
> ...
> I'm not saying you have to like those models.
> But the point is: there's something you look at and then you make up your mind whether or not you like those models; and then they're tested against data.
> So, when it comes to AGI and existential risk, it turns out as best I can ascertain, in the 20 years or so we've been talking about this seriously, there isn't a single model done."

He goes on:

> "I don't think any idea should be dismissed.
> I've just been inviting [AI doomsayers] to actually join the discourse of science.
> 'Show us your models.
> Let us see their assumptions and let's talk about those.'
> The practice, instead, is to write these very long pieces online, which just stack arguments vertically and raise the level of anxiety.
> ...
> Their mental model is so much: 'We're the insiders, we're the experts.'
> ...
> My mental model is: There's a thing, science.
> Try to publish this stuff in journals.
> Try to model it."
>

[Good models](/blog/judging-economic-models) don't need to be complete descriptions of reality.
But they *do* need to be logically consistent.
Their purpose is to make explicit the assumptions and premises underlying our intuitions.
Then we can subject those intuitions to formal scrutiny.

For example, suppose I think people should do X.
I write down a model of the process by which they decide what to do.
My model comprises a set of assumptions that imply X.
Now I ask:
Are my assumptions reasonable?
Do I believe them?
If not, then either
(i) people shouldn't do X or
(ii) they don't make decisions according to the process I've written down.
Both cases teach me something: my intuition is wrong!

Tyler wants AI doomsayers to go on similar intellectual journeys.
He wants to know: exactly what assumptions do they make when they say humanity is doomed?
What are the logical foundations of that claim?
Only by exposing those foundations can we test and revise them.
That's how science works.
We tell each other *how* we think so that we can debate *what* we think.
Models help us frame the debate.
Sure, [all models are wrong](https://en.wikipedia.org/wiki/All_models_are_wrong).
But you can't beat a model by waffling!
